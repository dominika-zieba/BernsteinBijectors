{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x after inverse (2,)\n",
      "x after inverse (2,)\n",
      "x after inverse (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x after inverse (2,)\n",
      "x after inverse (2,)\n",
      "x after inverse (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:01<?, ?it/s, loss=5.6639624]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x after inverse (2,)\n",
      "x after inverse (2,)\n",
      "x after inverse (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:11<?, ?it/s, loss=5.6639624]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 134\u001b[0m\n\u001b[1;32m    132\u001b[0m lossdict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    133\u001b[0m tepochs\u001b[38;5;241m.\u001b[39mset_postfix(ldict, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 134\u001b[0m params, opt_state \u001b[38;5;241m=\u001b[39m update(params, prng_key, opt_state)        \u001b[38;5;66;03m#take a step in direction of stepest descent (negative gradient)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m#every n epochs write training stats to a file and plot the current flow distribution\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/pjit.py:256\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 256\u001b[0m   outs, out_flat, out_tree, args_flat, jaxpr \u001b[38;5;241m=\u001b[39m _python_pjit_helper(\n\u001b[1;32m    257\u001b[0m       fun, infer_params_fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    258\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable(jaxpr)\n\u001b[1;32m    259\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(executable, out_tree, args_flat, out_flat)\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/pjit.py:167\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m   dispatch\u001b[38;5;241m.\u001b[39mcheck_arg(arg)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m pjit_p\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs_flat, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/core.py:2657\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2653\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2654\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2655\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2656\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_with_trace(top_trace, args, params)\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/core.py:389\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 389\u001b[0m   out \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mprocess_primitive(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mfull_raise, args), params)\n\u001b[1;32m    390\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/core.py:869\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;241m*\u001b[39mtracers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/pjit.py:1212\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1209\u001b[0m donated_argnums \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(donated_invars) \u001b[38;5;28;01mif\u001b[39;00m d]\n\u001b[1;32m   1210\u001b[0m has_explicit_sharding \u001b[38;5;241m=\u001b[39m _pjit_explicit_sharding(\n\u001b[1;32m   1211\u001b[0m     in_shardings, out_shardings, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xc\u001b[38;5;241m.\u001b[39m_xla\u001b[38;5;241m.\u001b[39mpjit(name, f, call_impl_cache_miss, [], [], donated_argnums,\n\u001b[1;32m   1213\u001b[0m                     tree_util\u001b[38;5;241m.\u001b[39mdispatch_registry,\n\u001b[1;32m   1214\u001b[0m                     _get_cpp_global_cache(has_explicit_sharding))(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/pjit.py:1196\u001b[0m, in \u001b[0;36m_pjit_call_impl.<locals>.call_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_impl_cache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs_, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_):\n\u001b[0;32m-> 1196\u001b[0m   out_flat, compiled \u001b[38;5;241m=\u001b[39m _pjit_call_impl_python(\n\u001b[1;32m   1197\u001b[0m       \u001b[38;5;241m*\u001b[39margs, jaxpr\u001b[38;5;241m=\u001b[39mjaxpr, in_shardings\u001b[38;5;241m=\u001b[39min_shardings,\n\u001b[1;32m   1198\u001b[0m       out_shardings\u001b[38;5;241m=\u001b[39mout_shardings, resource_env\u001b[38;5;241m=\u001b[39mresource_env,\n\u001b[1;32m   1199\u001b[0m       donated_invars\u001b[38;5;241m=\u001b[39mdonated_invars, name\u001b[38;5;241m=\u001b[39mname, keep_unused\u001b[38;5;241m=\u001b[39mkeep_unused,\n\u001b[1;32m   1200\u001b[0m       inline\u001b[38;5;241m=\u001b[39minline)\n\u001b[1;32m   1201\u001b[0m   fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m   1202\u001b[0m       compiled, tree_structure(out_flat), args, out_flat)\n\u001b[1;32m   1203\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out_flat, fastpath_data\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/pjit.py:1132\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _most_recent_pjit_call_executable\n\u001b[1;32m   1125\u001b[0m in_shardings \u001b[38;5;241m=\u001b[39m _resolve_in_shardings(\n\u001b[1;32m   1126\u001b[0m     args, in_shardings, out_shardings,\n\u001b[1;32m   1127\u001b[0m     resource_env\u001b[38;5;241m.\u001b[39mphysical_mesh \u001b[38;5;28;01mif\u001b[39;00m resource_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1129\u001b[0m compiled \u001b[38;5;241m=\u001b[39m _pjit_lower(\n\u001b[1;32m   1130\u001b[0m     jaxpr, in_shardings, out_shardings, resource_env,\n\u001b[1;32m   1131\u001b[0m     donated_invars, name, keep_unused, inline,\n\u001b[0;32m-> 1132\u001b[0m     lowering_parameters\u001b[38;5;241m=\u001b[39mmlir\u001b[38;5;241m.\u001b[39mLoweringParameters())\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[1;32m   1133\u001b[0m _most_recent_pjit_call_executable\u001b[38;5;241m.\u001b[39mweak_key_dict[jaxpr] \u001b[38;5;241m=\u001b[39m compiled\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:2276\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompile\u001b[39m(\u001b[38;5;28mself\u001b[39m, compiler_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MeshExecutable:\n\u001b[1;32m   2275\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2276\u001b[0m     executable \u001b[38;5;241m=\u001b[39m UnloadedMeshExecutable\u001b[38;5;241m.\u001b[39mfrom_hlo(\n\u001b[1;32m   2277\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hlo, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile_args,\n\u001b[1;32m   2278\u001b[0m         compiler_options\u001b[38;5;241m=\u001b[39mcompiler_options)\n\u001b[1;32m   2279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compiler_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2280\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:2624\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2621\u001b[0m       mesh \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mmesh  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2622\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2624\u001b[0m xla_executable, compile_options \u001b[38;5;241m=\u001b[39m _cached_compilation(\n\u001b[1;32m   2625\u001b[0m     hlo, name, mesh, spmd_lowering,\n\u001b[1;32m   2626\u001b[0m     tuple_args, auto_spmd_lowering, allow_prop_to_outputs,\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(host_callbacks), backend, da, pmap_nreps,\n\u001b[1;32m   2628\u001b[0m     compiler_options_keys, compiler_options_values)\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_replicated\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2631\u001b[0m   semantics_in_shardings \u001b[38;5;241m=\u001b[39m SemanticallyEqualShardings(in_shardings)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:2531\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, _allow_propagation_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values)\u001b[0m\n\u001b[1;32m   2526\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, compile_options\n\u001b[1;32m   2528\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2530\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2531\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m compiler\u001b[38;5;241m.\u001b[39mcompile_or_get_cached(\n\u001b[1;32m   2532\u001b[0m       backend, computation, dev, compile_options, host_callbacks)\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable, compile_options\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/compiler.py:294\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m use_compilation_cache \u001b[38;5;241m=\u001b[39m (compilation_cache\u001b[38;5;241m.\u001b[39mis_initialized() \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    291\u001b[0m                          backend\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;129;01min\u001b[39;00m supported_platforms)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_compilation_cache:\n\u001b[0;32m--> 294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend_compile(backend, computation, compile_options,\n\u001b[1;32m    295\u001b[0m                          host_callbacks)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# TODO(b/293308239) Instrument a metric to track the adoption of the new cache\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# key implementation after it is enabled.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _cache_used\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/.conda/envs/ripple_cuda/lib/python3.11/site-packages/jax/_src/compiler.py:256\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    252\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as pl\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'\n",
    "\n",
    "import haiku as hk\n",
    "import optax\n",
    "from typing import Any, Iterator, Mapping, Optional, Sequence, Tuple\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "\n",
    "# Define a target distribution (2D bimodial gaussian)\n",
    "def log_target(x):   \n",
    "    mean_1 = jnp.array([0.25,0.25])\n",
    "    cov_1 = jnp.array([[0.01,0],[0,0.01]])\n",
    "    mean_2 = jnp.array([0.75,0.25])\n",
    "    cov_2 =jnp.array([[0.01,0],[0,0.01]])\n",
    "    return jnp.log(jax.scipy.stats.multivariate_normal.pdf(x, mean_1, cov_1)+ jax.scipy.stats.multivariate_normal.pdf(x, mean_2, cov_2))\n",
    "\n",
    "#from vi_routines import make_flow_model\n",
    "from vi_routines_BP import make_flow_model\n",
    "\n",
    "# Training routines:\n",
    "\n",
    "#flow model\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def sample_and_log_prob(prng_key: PRNGKey, n: int) -> Tuple[Any, Array]:      # returns x (sample from the flow q), and model.log_prob(x) (array of log(q) of the sampled points)\n",
    "\n",
    "    model = make_flow_model(                          #this is the flow distribution (a distrax object)\n",
    "        event_shape=(n_params,),\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins\n",
    "    )\n",
    "\n",
    "    return model.sample_and_log_prob(seed=prng_key, sample_shape=(n,))\n",
    "\n",
    "#target\n",
    "def log_prob(x: Array) -> Array:\n",
    "    return log_target(x)\n",
    "\n",
    "#Loss - reverse KL between the flow and the target\n",
    "def loss_fn(params: hk.Params, prng_key: PRNGKey, n: int) -> Array:       #computes reverse KL-divergence for the sample x_flow between the flow and gw loglikelihood.\n",
    "    x_flow, log_q = sample_and_log_prob.apply(params, prng_key, n)           #gets sample from the flow and computes log_q for the sampled points.\n",
    "    log_p = log_prob(x_flow)      #gets log of target prob for the sampled points                                 \n",
    "    loss = jnp.mean(log_q - log_p)    #reverse KL\n",
    "    return loss\n",
    "\n",
    "#Gradient descent\n",
    "@jax.jit\n",
    "def update(                        #training update with stochastic gradient descent.\n",
    "    params: hk.Params,\n",
    "    prng_key: PRNGKey,\n",
    "    opt_state: OptState,\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, Nsamps) #gradient w.r.t. params, evalueated at params, prng_key, Nsamps.\n",
    "    updates, new_opt_state = optimiser.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "\n",
    "#Training log routines:\n",
    "def kl_ess(log_model_prob, log_target_prob):  #computes an evidence estimate, KL & effective sample size for logging. \n",
    "    weights = jnp.exp(log_target_prob - log_model_prob)   # sample weights, w = p/q\n",
    "    Z = jnp.mean(weights)  # evidence estimate\n",
    "    KL = jnp.mean(log_model_prob - log_target_prob) + jnp.log(Z)  # KL estimate taking into account evidence\n",
    "    ESS = jnp.sum(weights) ** 2 / jnp.sum(weights ** 2)      # effective sample size estimate\n",
    "    return Z, KL, ESS\n",
    "\n",
    "def init_logging(filename):\n",
    "    logf = open(filename, 'a')\n",
    "    fieldnames = ['iter', 'loss', 'kl', 'ess']\n",
    "    writer = csv.DictWriter(logf, fieldnames=fieldnames)\n",
    "    if os.stat(filename).st_size == 0:\n",
    "        writer.writeheader()\n",
    "        logf.flush()\n",
    "    return logf, writer\n",
    "\n",
    "#Training Loop\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #run_name = 'test_BP' #name of the run\n",
    "    #os.mkdir('results/test_splines')\n",
    "\n",
    "    #flow parameters\n",
    "    n_params = 2       #dimensionality of the domain\n",
    "    flow_num_layers = 3   \n",
    "    hidden_size = 128\n",
    "    mlp_num_layers = 2\n",
    "    num_bins = 10*3\n",
    "\n",
    "    #training parameters\n",
    "    epochs = 1000\n",
    "    Nsamps = 1000\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    optimiser = optax.adam(learning_rate)             #single optimiser.. #single network..\n",
    "\n",
    "\n",
    "    #initialise the flow\n",
    "    prng_seq = hk.PRNGSequence(42)\n",
    "    key = next(prng_seq)\n",
    "    params = sample_and_log_prob.init(key, prng_key=key, n=Nsamps)\n",
    "    opt_state = optimiser.init(params)\n",
    "\n",
    "    from tqdm import tqdm, trange\n",
    "    import corner\n",
    "\n",
    "    ldict = dict(loss = [])\n",
    "    lossdict = dict(loss = [])\n",
    "\n",
    "    #start logging:\n",
    "    #logf, writer = init_logging(f'results/{run_name}/log.csv')\n",
    "\n",
    "    #Training loop:\n",
    "    with trange(epochs) as tepochs:\n",
    "            for epoch in tepochs:\n",
    "                prng_key = next(prng_seq)\n",
    "                loss = loss_fn(params,  prng_key, Nsamps)\n",
    "                ldict['loss'] = loss\n",
    "                lossdict['loss'].append(loss)\n",
    "                tepochs.set_postfix(ldict, refresh=True)\n",
    "                params, opt_state = update(params, prng_key, opt_state)        #take a step in direction of stepest descent (negative gradient)\n",
    "\n",
    "                #every n epochs write training stats to a file and plot the current flow distribution\n",
    "                if (epoch)%20 == 0:\n",
    "                    print(f'Epoch {epoch}, loss {loss}')\n",
    "\n",
    "                    x_gen, log_flow = sample_and_log_prob.apply(params, next(prng_seq), Nsamps)\n",
    "                    log_p = log_prob(x_gen)\n",
    "                    _, kl, ess = kl_ess(log_flow, log_p)\n",
    "\n",
    "                    fig = corner.corner(np.array(x_gen, copy=False))\n",
    "                    pl.show()\n",
    "                    #pl.savefig(f'results/{run_name}/flow_{epoch}.png')\n",
    "                    pl.close()\n",
    "                \n",
    "                    ess = ess / Nsamps * 100  #percentage effective samples size \n",
    "\n",
    "                    #writer.writerow({'iter': epoch, 'loss': loss, 'kl': kl, 'ess': ess})\n",
    "                    #logf.flush()\n",
    "                \n",
    "    print(\"Done!\")  \n",
    "\n",
    "    #Save results:\n",
    "\n",
    "    x_gen, log_flow = sample_and_log_prob.apply(params, next(prng_seq), Nsamps)\n",
    "\n",
    "    fig = corner.corner(np.array(x_gen, copy=False))\n",
    "    pl.savefig(f'results/{run_name}/posterior_flow.png')\n",
    "    pl.close()\n",
    "\n",
    "    L=np.array(lossdict['loss'])\n",
    "    pl.plot(L)\n",
    "    pl.xlabel(\"Iteration\")\n",
    "    pl.ylabel(\"Loss\")\n",
    "    pl.savefig(f'results/{run_name}/loss.png')\n",
    "    pl.close()\n",
    "\n",
    "    f = open(f'results/{run_name}/loss.npy', 'wb')\n",
    "    np.save(f,L)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ripple_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

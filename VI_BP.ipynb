{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "  0%|          | 0/1000 [00:06<?, ?it/s, loss=-54.325497]"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as pl\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'\n",
    "\n",
    "import haiku as hk\n",
    "import optax\n",
    "from typing import Any, Iterator, Mapping, Optional, Sequence, Tuple\n",
    "Array = jnp.ndarray\n",
    "PRNGKey = Array\n",
    "OptState = Any\n",
    "\n",
    "\n",
    "\n",
    "# Define a target distribution (2D bimodial gaussian)\n",
    "#def log_target(x):   \n",
    "#    mean_1 = jnp.array([0.25,0.25])\n",
    "#    cov_1 = jnp.array([[0.01,0],[0,0.01]])\n",
    "#    mean_2 = jnp.array([0.75,0.25])\n",
    "#    cov_2 =jnp.array([[0.01,0],[0,0.01]])\n",
    "#    return jnp.log(jax.scipy.stats.multivariate_normal.pdf(x, mean_1, cov_1)+ jax.scipy.stats.multivariate_normal.pdf(x, mean_2, cov_2))\n",
    "\n",
    "def log_target(x):\n",
    "    x = x.T\n",
    "    return  (2 + jnp.cos(x[0]*10*np.pi/2)*jnp.cos(x[1]*10*np.pi/2))**5 \n",
    "\n",
    "#from vi_routines import make_flow_model\n",
    "from vi_routines_BP import make_flow_model\n",
    "\n",
    "# Training routines:\n",
    "\n",
    "#flow model\n",
    "@hk.without_apply_rng\n",
    "@hk.transform\n",
    "def sample_and_log_prob(prng_key: PRNGKey, n: int) -> Tuple[Any, Array]:      # returns x (sample from the flow q), and model.log_prob(x) (array of log(q) of the sampled points)\n",
    "\n",
    "    model = make_flow_model(                          #this is the flow distribution (a distrax object)\n",
    "        event_shape=(n_params,),\n",
    "        num_layers=flow_num_layers,\n",
    "        hidden_sizes=[hidden_size] * mlp_num_layers,\n",
    "        num_bins=num_bins\n",
    "    )\n",
    "\n",
    "    return model.sample_and_log_prob(seed=prng_key, sample_shape=(n,))\n",
    "\n",
    "#target\n",
    "def log_prob(x: Array) -> Array:\n",
    "    return log_target(x)\n",
    "\n",
    "#Loss - reverse KL between the flow and the target\n",
    "def loss_fn(params: hk.Params, prng_key: PRNGKey, n: int) -> Array:       #computes reverse KL-divergence for the sample x_flow between the flow and gw loglikelihood.\n",
    "    x_flow, log_q = sample_and_log_prob.apply(params, prng_key, n)           #gets sample from the flow and computes log_q for the sampled points.\n",
    "    log_p = log_prob(x_flow)      #gets log of target prob for the sampled points                                 \n",
    "    loss = jnp.mean(log_q - log_p)    #reverse KL\n",
    "    return loss\n",
    "\n",
    "#Gradient descent\n",
    "@jax.jit\n",
    "def update(                        #training update with stochastic gradient descent.\n",
    "    params: hk.Params,\n",
    "    prng_key: PRNGKey,\n",
    "    opt_state: OptState,\n",
    ") -> Tuple[hk.Params, OptState]:\n",
    "    \"\"\"Single SGD update step.\"\"\"\n",
    "    grads = jax.grad(loss_fn)(params, prng_key, Nsamps) #gradient w.r.t. params, evalueated at params, prng_key, Nsamps.\n",
    "    updates, new_opt_state = optimiser.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state\n",
    "\n",
    "\n",
    "#Training log routines:\n",
    "def kl_ess(log_model_prob, log_target_prob):  #computes an evidence estimate, KL & effective sample size for logging. \n",
    "    weights = jnp.exp(log_target_prob - log_model_prob)   # sample weights, w = p/q\n",
    "    Z = jnp.mean(weights)  # evidence estimate\n",
    "    KL = jnp.mean(log_model_prob - log_target_prob) + jnp.log(Z)  # KL estimate taking into account evidence\n",
    "    ESS = jnp.sum(weights) ** 2 / jnp.sum(weights ** 2)      # effective sample size estimate\n",
    "    return Z, KL, ESS\n",
    "\n",
    "def init_logging(filename):\n",
    "    logf = open(filename, 'a')\n",
    "    fieldnames = ['iter', 'loss', 'kl', 'ess']\n",
    "    writer = csv.DictWriter(logf, fieldnames=fieldnames)\n",
    "    if os.stat(filename).st_size == 0:\n",
    "        writer.writeheader()\n",
    "        logf.flush()\n",
    "    return logf, writer\n",
    "\n",
    "#Training Loop\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #run_name = 'test_BP' #name of the run\n",
    "    #os.mkdir('results/test_splines')\n",
    "\n",
    "    #flow parameters\n",
    "    n_params = 2       #dimensionality of the domain\n",
    "    flow_num_layers = 4   \n",
    "    hidden_size = 128\n",
    "    mlp_num_layers = 4\n",
    "    num_bins = 100\n",
    "\n",
    "    #training parameters\n",
    "    epochs = 1000\n",
    "    Nsamps = 1000\n",
    "\n",
    "    learning_rate = 0.0001\n",
    "    optimiser = optax.adam(learning_rate)             #single optimiser.. #single network..\n",
    "\n",
    "\n",
    "    #initialise the flow\n",
    "    prng_seq = hk.PRNGSequence(42)\n",
    "    key = next(prng_seq)\n",
    "    params = sample_and_log_prob.init(key, prng_key=key, n=Nsamps)\n",
    "    opt_state = optimiser.init(params)\n",
    "\n",
    "    from tqdm import tqdm, trange\n",
    "    import corner\n",
    "\n",
    "    ldict = dict(loss = [])\n",
    "    lossdict = dict(loss = [])\n",
    "\n",
    "    #start logging:\n",
    "    #logf, writer = init_logging(f'results/{run_name}/log.csv')\n",
    "\n",
    "    #Training loop:\n",
    "    with trange(epochs) as tepochs:\n",
    "            for epoch in tepochs:\n",
    "                prng_key = next(prng_seq)\n",
    "                loss = loss_fn(params,  prng_key, Nsamps)\n",
    "                ldict['loss'] = loss\n",
    "                lossdict['loss'].append(loss)\n",
    "                tepochs.set_postfix(ldict, refresh=True)\n",
    "                params, opt_state = update(params, prng_key, opt_state)        #take a step in direction of stepest descent (negative gradient)\n",
    "\n",
    "                #every n epochs write training stats to a file and plot the current flow distribution\n",
    "                if (epoch)%20 == 0:\n",
    "                    print(f'Epoch {epoch}, loss {loss}')\n",
    "\n",
    "                    x_gen, log_flow = sample_and_log_prob.apply(params, next(prng_seq), Nsamps)\n",
    "                    log_p = log_prob(x_gen)\n",
    "                    _, kl, ess = kl_ess(log_flow, log_p)\n",
    "\n",
    "                    fig = corner.corner(np.array(x_gen, copy=False))\n",
    "                    pl.show()\n",
    "                    #pl.savefig(f'results/{run_name}/flow_{epoch}.png')\n",
    "                    pl.close()\n",
    "                \n",
    "                    ess = ess / Nsamps * 100  #percentage effective samples size \n",
    "\n",
    "                    #writer.writerow({'iter': epoch, 'loss': loss, 'kl': kl, 'ess': ess})\n",
    "                    #logf.flush()\n",
    "                \n",
    "    print(\"Done!\")  \n",
    "\n",
    "    #Save results:\n",
    "\n",
    "    x_gen, log_flow = sample_and_log_prob.apply(params, next(prng_seq), Nsamps)\n",
    "\n",
    "    fig = corner.corner(np.array(x_gen, copy=False))\n",
    "    pl.savefig(f'results/{run_name}/posterior_flow.png')\n",
    "    pl.close()\n",
    "\n",
    "    L=np.array(lossdict['loss'])\n",
    "    pl.plot(L)\n",
    "    pl.xlabel(\"Iteration\")\n",
    "    pl.ylabel(\"Loss\")\n",
    "    pl.savefig(f'results/{run_name}/loss.png')\n",
    "    pl.close()\n",
    "\n",
    "    f = open(f'results/{run_name}/loss.npy', 'wb')\n",
    "    np.save(f,L)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ripple_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
